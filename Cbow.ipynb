{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71bab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Lambda, Dense\n",
    "from keras.utils import to_categorical  # Corrected import here\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc4e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data (you can replace this with a larger corpus or read from a file)\n",
    "data = \"\"\"\n",
    "The speed of transmission is an important point of difference between the two viruses. \n",
    "Influenza has a shorter median incubation period and a shorter serial interval than COVID-19.\n",
    "The serial interval for COVID-19 is estimated to be 5-6 days, while for influenza, it is 3 days.\n",
    "Influenza can spread faster than COVID-19.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52b38dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocess text data - Convert text to lowercase\n",
    "data = data.lower().split()  # Convert to lowercase and split into words (tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ca4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text (assign a unique index to each word)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])  # Fit the tokenizer on the text data\n",
    "word_to_index = tokenizer.word_index  # Mapping of words to indices\n",
    "word_to_index['PAD'] = 0  # Add padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ba9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse mapping (index to word)\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d7078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the words into their respective indices\n",
    "encoded_data = [word_to_index[word] for word in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b56532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "vocab_size = len(word_to_index)  # Vocabulary size\n",
    "embed_size = 100  # Size of word embeddings\n",
    "window_size = 2  # Number of words before and after the target word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf83449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare context-target pairs\n",
    "def generate_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2  # Total number of words in context\n",
    "\n",
    "    for index, word in enumerate(corpus):\n",
    "        # Context window\n",
    "        start = max(0, index - window_size)\n",
    "        end = min(len(corpus), index + window_size + 1)\n",
    "        \n",
    "        # Skip the word itself (target word)\n",
    "        context = [corpus[i] for i in range(start, end) if i != index]\n",
    "        \n",
    "        # One-hot encode the target word\n",
    "        target = to_categorical([corpus[index]], vocab_size)  # Use to_categorical here\n",
    "        \n",
    "        # Pad context if necessary\n",
    "        context_padded = pad_sequences([context], maxlen=context_length, padding='post')\n",
    "        \n",
    "        yield (context_padded, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d404948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define the CBOW model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size * 2))  # Embedding layer\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))  # CBOW: Take mean of context words\n",
    "model.add(Dense(vocab_size, activation='softmax'))  # Output layer (softmax for classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d4154ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53265e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Epoch 1, Loss: 192.10788702964783\n",
      "Epoch 2, Loss: 190.26115560531616\n",
      "Epoch 3, Loss: 188.67545676231384\n",
      "Epoch 4, Loss: 187.01941227912903\n",
      "Epoch 5, Loss: 185.26665496826172\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train the model\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for context, target in generate_pairs(encoded_data, window_size, vocab_size):\n",
    "        loss = model.train_on_batch(context, target)\n",
    "        total_loss += loss\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb07480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (39, 100)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Extract word embeddings and calculate similarities\n",
    "embeddings = model.get_weights()[0][1:]  # Extract word embeddings (ignore padding token)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda02590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Euclidean distances between word vectors\n",
    "distance_matrix = euclidean_distances(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aff7e35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word to find similar words: ganesh\n",
      "'ganesh' not found in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Find similar words\n",
    "search_term = input(\"Enter a word to find similar words: \").strip().lower()  # Lowercase the search term\n",
    "if search_term in word_to_index:\n",
    "    index = word_to_index[search_term] - 1  # Adjust for padding token\n",
    "    similar_words = [index_to_word[idx] for idx in distance_matrix[index].argsort()[1:6]]\n",
    "    print(f\"Similar words to '{search_term}': {similar_words}\")\n",
    "else:\n",
    "    print(f\"'{search_term}' not found in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa204a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
